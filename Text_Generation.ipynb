{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA1mgxuPWXJ61khUAxEzLW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pakhi27/Text-Generation/blob/main/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using Gradio to wrap a text to text interface around GPT-J-6B"
      ],
      "metadata": {
        "id": "2_GTL-YzDfu8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8AZaGomDODa",
        "outputId": "4dbc34dd-3a94-473d-e421-3874aab53339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m677.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.5/93.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 (Generative Pre-trained Transformer 2) is a language model developed by OpenAI. It's a type of transformer-based model designed for generating human-like text based on a given input. Here’s a detailed explanation of how GPT-2 works and how it generates text:\n",
        "\n",
        "### **What is GPT-2?**\n",
        "\n",
        "1. **Architecture:**\n",
        "   - **Transformer Model:** GPT-2 is based on the Transformer architecture, which uses self-attention mechanisms to process and generate text. Transformers are highly effective for NLP tasks because they can capture long-range dependencies in text.\n",
        "   - **Pre-trained Model:** GPT-2 is pre-trained on a diverse range of internet text. This means it has learned patterns, grammar, facts about the world, and some reasoning abilities from this extensive dataset.\n",
        "\n",
        "2. **Size and Variants:**\n",
        "   - **Model Size:** GPT-2 comes in different sizes, with varying numbers of parameters (e.g., 124M, 355M, 774M, and 1.5B). Larger models generally have better performance but require more computational resources.\n",
        "   - **Parameters:** The size of the model (number of parameters) affects its ability to generate coherent and contextually appropriate text.\n",
        "\n",
        "### **How GPT-2 Generates Text**\n",
        "\n",
        "1. **Tokenization:**\n",
        "   - **Input Processing:** The input text is first tokenized using the GPT-2 tokenizer. Tokenization splits the text into subwords or word pieces that the model can understand. For example, \"OpenAI\" might be split into \"Open\" and \"AI\".\n",
        "   - **Input IDs:** These tokens are converted into numerical IDs, which are used as input to the model.\n",
        "\n",
        "2. **Text Generation Process:**\n",
        "   - **Contextual Understanding:** GPT-2 uses self-attention to understand the context of the input tokens. It generates text based on the patterns it learned during training.\n",
        "   - **Beam Search/ Sampling:** The model generates text in a step-by-step fashion. During each step, it predicts the next token based on the previous tokens. The methods used for generation include:\n",
        "     - **Beam Search:** Considers multiple possible sequences and selects the most likely one.\n",
        "     - **Sampling:** Picks tokens based on probabilities, introducing randomness.\n",
        "     - **Top-k Sampling:** Limits choices to the top k most probable tokens.\n",
        "     - **Top-p (Nucleus) Sampling:** Chooses tokens from the smallest possible set whose cumulative probability exceeds a threshold p.\n",
        "\n",
        "3. **Generation Criteria:**\n",
        "   - **Max Length:** The maximum number of tokens the model will generate.\n",
        "   - **No Repeat N-Grams:** Ensures that no repeated sequences of a certain length appear in the generated text.\n",
        "   - **Temperature:** Controls the randomness of predictions. A lower temperature makes the model more confident and deterministic, while a higher temperature increases randomness and creativity.\n",
        "\n",
        "4. **Output Processing:**\n",
        "   - **Decoding:** The generated token IDs are converted back into text using the tokenizer’s decode method. This text is cleaned and formatted to be human-readable.\n",
        "\n",
        "### **Text Generation Example**\n",
        "\n",
        "Given an input prompt, GPT-2:\n",
        "1. **Encodes** the input text into tokens.\n",
        "2. **Generates** a sequence of tokens by predicting one token at a time based on the input and previously generated tokens.\n",
        "3. **Decodes** the sequence of tokens back into human-readable text.\n",
        "\n",
        "### **Applications**\n",
        "\n",
        "GPT-2 can be used for various NLP tasks, including:\n",
        "- **Text Generation:** Creating coherent and contextually relevant text.\n",
        "- **Text Completion:** Completing partially written sentences or paragraphs.\n",
        "- **Translation:** Translating text between languages (though it’s not its primary function).\n",
        "- **Summarization:** Summarizing long documents.\n",
        "\n",
        "GPT-2’s ability to generate text relies on the patterns it has learned from its training data, allowing it to produce text that is contextually appropriate and coherent based on the given input."
      ],
      "metadata": {
        "id": "GC90osc4ONyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n"
      ],
      "metadata": {
        "id": "eSpOh5IDLdmE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "yxXzivXoOFA5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Defines a function generate_text that takes input_text as input.\n",
        "# input_ids = tokenizer.encode(input_text, return_tensors='pt'): Converts the input text into token IDs using the tokenizer. return_tensors='pt' specifies that the output should be in PyTorch tensor format.\n",
        "# beam_output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True): Generates text based on the input token IDs using beam search.\n",
        "# max_length=100: Limits the length of the generated text to 100 tokens.\n",
        "# num_beams=5: Uses beam search with 5 beams to find the best output.\n",
        "# no_repeat_ngram_size=2: Ensures that no 2-grams are repeated in the generated text.\n",
        "# early_stopping=True: Stops the generation when the model is confident enough to finish the sequence.\n",
        "def generate_text(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')  # Use 'pt' for PyTorch\n",
        "    beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=200,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    temperature=0.5,  # Adjust for creativity\n",
        "    top_k=50,  # Top-k sampling\n",
        "    top_p=0.95  # Top-p sampling (nucleus sampling)\n",
        ")\n",
        "     # Converts the generated token IDs back into human-readable text, skipping special tokens and cleaning up unnecessary spaces.\n",
        "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    # \": Joins the generated sentences, removes the last sentence fragment if it's incomplete, and adds a period at the end.\n",
        "    return \".\".join(output.split(\".\")[:-1]) + \".\"\n"
      ],
      "metadata": {
        "id": "3IDb4vTwLdoz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Gradio interface\n",
        "# interface = gr.Interface(: Creates a Gradio interface for the text generation function.\n",
        "# fn=generate_text: Specifies the function to be called when input is provided.\n",
        "# inputs=gr.Textbox(label=\"Input Text\"): Defines a textbox as the input interface for the user to enter text.\n",
        "# outputs=gr.Textbox(label=\"Output Text\"): Defines a textbox to display the generated text.\n",
        "# title=\"GPT-2 Text Generator\": Sets the title of the interface.\n",
        "# description=\"OpenAI's GPT-2 is an unsupervised language model that can generate coherent text. Input a sentence and see what it completes it with! Takes around 20s to run!\": Provides a description of the interface.\n",
        "interface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=gr.Textbox(label=\"Input Text\"),\n",
        "    outputs=gr.Textbox(label=\"Output Text\"),\n",
        "    title=\"GPT-2 Text Generator\",\n",
        "    description=\"OpenAI's GPT-2 is an unsupervised language model that can generate coherent text. Input a sentence and see what it completes it with! Takes around 20s to run!\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "D6GFvAPcLdrR",
        "outputId": "b9152649-7605-42fe-cd7b-9ae0ede8f402"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://23111908891f279c3d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://23111908891f279c3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsXDZGWVFMJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APTT4JcMFMM_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}